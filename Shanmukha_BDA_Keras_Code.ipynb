{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shannu1922-ai/ICP/blob/main/Shanmukha_BDA_Keras_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Simple Neural Network with Keras Sequential API***"
      ],
      "metadata": {
        "id": "f8QdGW-45_Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Generate some random data\n",
        "x_train = np.random.random((1000, 20))  # 1000 samples, 20 features each\n",
        "y_train = np.random.randint(2, size=(1000, 1))  # Binary labels (0 or 1)\n",
        "\n",
        "x_test = np.random.random((200, 20))  # 200 test samples\n",
        "y_test = np.random.randint(2, size=(200, 1))  # Binary labels for testing\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a hidden layer with 64 neurons and ReLU activation\n",
        "model.add(Dense(64, activation='relu', input_shape=(20,)))\n",
        "\n",
        "# Add another hidden layer with 32 neurons and ReLU activation\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Add an output layer with 1 neuron and sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJpPAbBG6LWQ",
        "outputId": "d50948e9-e573-435b-c689-4a97ef16e87b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4944 - loss: 0.6993\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5625 - loss: 0.6863\n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5770 - loss: 0.6843\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5318 - loss: 0.6858\n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5522 - loss: 0.6844\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5750 - loss: 0.6758  \n",
            "Test accuracy: 0.5649999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sequential API to create and train a neural network for classifying the MNIST dataset.***"
      ],
      "metadata": {
        "id": "ske7X0_r33aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: normalize images and one-hot encode labels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input (28x28 images) into a vector of size 784\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add a hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Add the output layer with 10 neurons (one for each class) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygO844pd3anG",
        "outputId": "c123691c-a9e3-4475-ba14-f8b8feee719b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8708 - loss: 0.4672 - val_accuracy: 0.9583 - val_loss: 0.1498\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9628 - loss: 0.1324 - val_accuracy: 0.9665 - val_loss: 0.1130\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9755 - loss: 0.0863 - val_accuracy: 0.9659 - val_loss: 0.1100\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9805 - loss: 0.0647 - val_accuracy: 0.9734 - val_loss: 0.0889\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.0458 - val_accuracy: 0.9742 - val_loss: 0.0885\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9897 - loss: 0.0351 - val_accuracy: 0.9730 - val_loss: 0.0912\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9905 - loss: 0.0309 - val_accuracy: 0.9732 - val_loss: 0.0946\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9940 - loss: 0.0222 - val_accuracy: 0.9736 - val_loss: 0.0980\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9959 - loss: 0.0163 - val_accuracy: 0.9724 - val_loss: 0.1026\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9951 - loss: 0.0152 - val_accuracy: 0.9737 - val_loss: 0.0974\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.1087\n",
            "Test accuracy: 0.9750000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used 5 hidden layers and used ReLU for activation fuction. for the no of neurons or nodes I used 512, 256, 128, 64 and 32 respectively for the 5 hidden layers. the optimizer is 'adam'."
      ],
      "metadata": {
        "id": "CIZ6QF5-hm7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: normalize images and one-hot encode labels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input (28x28 images) into a vector of size 784\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add a hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Add the output layer with 10 neurons (one for each class) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "id": "H4VP_F0d3paH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7482245-12b2-490c-ae9d-f828265b62dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.8602 - loss: 0.4478 - val_accuracy: 0.9633 - val_loss: 0.1252\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9683 - loss: 0.1078 - val_accuracy: 0.9649 - val_loss: 0.1172\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.9785 - loss: 0.0718 - val_accuracy: 0.9692 - val_loss: 0.1278\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9827 - loss: 0.0556 - val_accuracy: 0.9680 - val_loss: 0.1231\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.9866 - loss: 0.0454 - val_accuracy: 0.9753 - val_loss: 0.0959\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9888 - loss: 0.0383 - val_accuracy: 0.9686 - val_loss: 0.1334\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9887 - loss: 0.0380 - val_accuracy: 0.9768 - val_loss: 0.0842\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9920 - loss: 0.0276 - val_accuracy: 0.9762 - val_loss: 0.1016\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9928 - loss: 0.0251 - val_accuracy: 0.9779 - val_loss: 0.0940\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9922 - loss: 0.0248 - val_accuracy: 0.9688 - val_loss: 0.1444\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9677 - loss: 0.1376\n",
            "Test accuracy: 0.9728999733924866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I added the dropout method with a value of 0.2 for the hidden values, I also used 'rmsprop' as my optimizer\n"
      ],
      "metadata": {
        "id": "qT-aASaqiSsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: normalize images and one-hot encode labels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input (28x28 images) into a vector of size 784\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add a hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Add the output layer with 10 neurons (one for each class) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gb19bTjJ4d9",
        "outputId": "7ee71d3a-9296-4941-b6da-4d8b9b8b391d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.8143 - loss: 0.5910 - val_accuracy: 0.9597 - val_loss: 0.1477\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 11ms/step - accuracy: 0.9554 - loss: 0.1647 - val_accuracy: 0.9644 - val_loss: 0.1396\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9674 - loss: 0.1289 - val_accuracy: 0.9728 - val_loss: 0.1131\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9709 - loss: 0.1145 - val_accuracy: 0.9652 - val_loss: 0.1721\n",
            "Epoch 5/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9773 - loss: 0.0978 - val_accuracy: 0.9738 - val_loss: 0.1340\n",
            "Epoch 6/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9759 - loss: 0.1090 - val_accuracy: 0.9759 - val_loss: 0.1343\n",
            "Epoch 7/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9775 - loss: 0.1039 - val_accuracy: 0.9758 - val_loss: 0.1385\n",
            "Epoch 8/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9786 - loss: 0.1084 - val_accuracy: 0.9741 - val_loss: 0.1480\n",
            "Epoch 9/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9793 - loss: 0.1066 - val_accuracy: 0.9737 - val_loss: 0.1891\n",
            "Epoch 10/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9799 - loss: 0.1120 - val_accuracy: 0.9772 - val_loss: 0.1427\n",
            "Epoch 11/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9805 - loss: 0.1071 - val_accuracy: 0.9772 - val_loss: 0.1388\n",
            "Epoch 12/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9817 - loss: 0.0979 - val_accuracy: 0.9778 - val_loss: 0.1898\n",
            "Epoch 13/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9815 - loss: 0.1084 - val_accuracy: 0.9747 - val_loss: 0.1762\n",
            "Epoch 14/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9816 - loss: 0.1135 - val_accuracy: 0.9778 - val_loss: 0.2215\n",
            "Epoch 15/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9805 - loss: 0.1237 - val_accuracy: 0.9758 - val_loss: 0.1990\n",
            "Epoch 16/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.9823 - loss: 0.1146 - val_accuracy: 0.9769 - val_loss: 0.2103\n",
            "Epoch 17/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9807 - loss: 0.1281 - val_accuracy: 0.9737 - val_loss: 0.2117\n",
            "Epoch 18/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9828 - loss: 0.1162 - val_accuracy: 0.9759 - val_loss: 0.2445\n",
            "Epoch 19/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9824 - loss: 0.1224 - val_accuracy: 0.9756 - val_loss: 0.2536\n",
            "Epoch 20/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9825 - loss: 0.1257 - val_accuracy: 0.9749 - val_loss: 0.1754\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9749 - loss: 0.1676\n",
            "Test accuracy: 0.9775999784469604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have just changes the optimiser to 'sgd' but it gave me a good result compared to 'rmsprop'"
      ],
      "metadata": {
        "id": "tzs-Vqt1jk7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: normalize images and one-hot encode labels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input (28x28 images) into a vector of size 784\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# Add a hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "# Add the output layer with 10 neurons (one for each class) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxW8i3tcMgHH",
        "outputId": "86633ac5-c5c1-4733-f40b-57704dcb3f4a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.4140 - loss: 1.6717 - val_accuracy: 0.9087 - val_loss: 0.3141\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.8783 - loss: 0.4057 - val_accuracy: 0.9366 - val_loss: 0.2169\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9166 - loss: 0.2832 - val_accuracy: 0.9494 - val_loss: 0.1695\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9352 - loss: 0.2207 - val_accuracy: 0.9589 - val_loss: 0.1398\n",
            "Epoch 5/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9442 - loss: 0.1833 - val_accuracy: 0.9623 - val_loss: 0.1260\n",
            "Epoch 6/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9518 - loss: 0.1619 - val_accuracy: 0.9657 - val_loss: 0.1160\n",
            "Epoch 7/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9595 - loss: 0.1342 - val_accuracy: 0.9680 - val_loss: 0.1059\n",
            "Epoch 8/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9634 - loss: 0.1224 - val_accuracy: 0.9687 - val_loss: 0.1064\n",
            "Epoch 9/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9652 - loss: 0.1134 - val_accuracy: 0.9722 - val_loss: 0.0966\n",
            "Epoch 10/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9680 - loss: 0.1014 - val_accuracy: 0.9728 - val_loss: 0.0916\n",
            "Epoch 11/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9723 - loss: 0.0906 - val_accuracy: 0.9747 - val_loss: 0.0922\n",
            "Epoch 12/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9737 - loss: 0.0849 - val_accuracy: 0.9756 - val_loss: 0.0868\n",
            "Epoch 13/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9765 - loss: 0.0771 - val_accuracy: 0.9745 - val_loss: 0.0846\n",
            "Epoch 14/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9787 - loss: 0.0691 - val_accuracy: 0.9762 - val_loss: 0.0825\n",
            "Epoch 15/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9790 - loss: 0.0641 - val_accuracy: 0.9763 - val_loss: 0.0823\n",
            "Epoch 16/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9813 - loss: 0.0609 - val_accuracy: 0.9767 - val_loss: 0.0831\n",
            "Epoch 17/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9814 - loss: 0.0583 - val_accuracy: 0.9785 - val_loss: 0.0800\n",
            "Epoch 18/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9830 - loss: 0.0530 - val_accuracy: 0.9771 - val_loss: 0.0825\n",
            "Epoch 19/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0468 - val_accuracy: 0.9789 - val_loss: 0.0782\n",
            "Epoch 20/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.9857 - loss: 0.0449 - val_accuracy: 0.9779 - val_loss: 0.0791\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0841\n",
            "Test accuracy: 0.9789000153541565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im going to try the above code with 'adam' as the optimiser. previously I've used adam but not with the dropouts. it gave a slight better improvement compared to 'sgd'.\n",
        "\n",
        "I have also used Early stopping option for this one, the early stopping will stop the training automatically if it detects that the model is not improving. it reduces the defect of overfitting."
      ],
      "metadata": {
        "id": "-5VIAx4ckG5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input,Dense, Flatten, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data: normalize images and one-hot encode labels\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Flatten the input (28x28 images) into a vector of size 784\n",
        "model.add(Input(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "# Add the output layer with 10 neurons (one for each class) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Early Stopping\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GO8YIFROHDw",
        "outputId": "6f39e4b9-d517-4320-b7bc-0a5e9a7b4d69"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.8081 - loss: 0.6005 - val_accuracy: 0.9609 - val_loss: 0.1479\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9557 - loss: 0.1521 - val_accuracy: 0.9657 - val_loss: 0.1178\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9669 - loss: 0.1167 - val_accuracy: 0.9711 - val_loss: 0.1053\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9741 - loss: 0.0926 - val_accuracy: 0.9717 - val_loss: 0.1011\n",
            "Epoch 5/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9781 - loss: 0.0764 - val_accuracy: 0.9724 - val_loss: 0.1046\n",
            "Epoch 6/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9809 - loss: 0.0679 - val_accuracy: 0.9755 - val_loss: 0.0981\n",
            "Epoch 7/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9829 - loss: 0.0595 - val_accuracy: 0.9784 - val_loss: 0.0808\n",
            "Epoch 8/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9851 - loss: 0.0513 - val_accuracy: 0.9768 - val_loss: 0.1080\n",
            "Epoch 9/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - accuracy: 0.9855 - loss: 0.0481 - val_accuracy: 0.9742 - val_loss: 0.0958\n",
            "Epoch 10/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9869 - loss: 0.0449 - val_accuracy: 0.9769 - val_loss: 0.0898\n",
            "Epoch 11/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9887 - loss: 0.0394 - val_accuracy: 0.9769 - val_loss: 0.0935\n",
            "Epoch 12/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9874 - loss: 0.0422 - val_accuracy: 0.9779 - val_loss: 0.0956\n",
            "Epoch 13/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.9894 - loss: 0.0359 - val_accuracy: 0.9793 - val_loss: 0.0895\n",
            "Epoch 14/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9897 - loss: 0.0358 - val_accuracy: 0.9816 - val_loss: 0.0877\n",
            "Epoch 15/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.9909 - loss: 0.0328 - val_accuracy: 0.9772 - val_loss: 0.1061\n",
            "Epoch 16/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9908 - loss: 0.0335 - val_accuracy: 0.9781 - val_loss: 0.1054\n",
            "Epoch 17/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.9921 - loss: 0.0316 - val_accuracy: 0.9778 - val_loss: 0.1071\n",
            "Epoch 18/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9929 - loss: 0.0263 - val_accuracy: 0.9768 - val_loss: 0.1123\n",
            "Epoch 19/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9917 - loss: 0.0284 - val_accuracy: 0.9800 - val_loss: 0.1028\n",
            "Epoch 20/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.9928 - loss: 0.0257 - val_accuracy: 0.9786 - val_loss: 0.1050\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9764 - loss: 0.1297\n",
            "Test accuracy: 0.9807999730110168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this one I have reshaped the data which helps in the better use of Conv2D and MaxPooling2D functions\n",
        "\n",
        "Conv2D: this helps in extracting the patterns like edges and curves.\n",
        "\n",
        "MaxPooling2D: This helps in reducing the size of the data or desampling, thus increases the effieciency and reduces the runtime.\n",
        "\n",
        "I have used adam as the optimiser and the split I have made as 90-10 split"
      ],
      "metadata": {
        "id": "qYpwMFfukblZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Load and preprocess the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Regularization\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95kbpvs7QkU7",
        "outputId": "6e499745-2eb6-4d3d-beec-b430a437aa65"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 54ms/step - accuracy: 0.7789 - loss: 0.6803 - val_accuracy: 0.9808 - val_loss: 0.0681\n",
            "Epoch 2/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.9546 - loss: 0.1545 - val_accuracy: 0.9850 - val_loss: 0.0509\n",
            "Epoch 3/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.9652 - loss: 0.1184 - val_accuracy: 0.9872 - val_loss: 0.0422\n",
            "Epoch 4/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.9729 - loss: 0.0889 - val_accuracy: 0.9903 - val_loss: 0.0371\n",
            "Epoch 5/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.9751 - loss: 0.0802 - val_accuracy: 0.9890 - val_loss: 0.0418\n",
            "Epoch 6/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 56ms/step - accuracy: 0.9793 - loss: 0.0683 - val_accuracy: 0.9898 - val_loss: 0.0448\n",
            "Epoch 7/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 52ms/step - accuracy: 0.9801 - loss: 0.0638 - val_accuracy: 0.9912 - val_loss: 0.0370\n",
            "Epoch 8/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.9844 - loss: 0.0524 - val_accuracy: 0.9918 - val_loss: 0.0348\n",
            "Epoch 9/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.9834 - loss: 0.0492 - val_accuracy: 0.9910 - val_loss: 0.0390\n",
            "Epoch 10/10\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.9858 - loss: 0.0459 - val_accuracy: 0.9913 - val_loss: 0.0365\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9880 - loss: 0.0360\n",
            "Test accuracy: 0.9905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are not supposed to use the Conv2D and the MaxPooling2D methods.\n",
        "\n",
        "I have made some changes to the previous model but I used BatchNormalization() which helps in normalizing the batch which helps in the increase of accuracy.\n",
        "\n",
        "I have added the learning rate for this, I have increased the no of neurons for each hidden layer and I have also increase the size of batches to 64. and epochs to 30."
      ],
      "metadata": {
        "id": "SLKHoDDpmKdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load and preprocess the data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a sequential model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Hidden layers\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "#Outpuit layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Early stopping\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'\\nTest Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBeEsCocUpPt",
        "outputId": "1c329bda-ade6-4bfd-88cb-04debdaebf53"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "844/844 - 32s - 37ms/step - accuracy: 0.8993 - loss: 0.3262 - val_accuracy: 0.9717 - val_loss: 0.0938\n",
            "Epoch 2/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9540 - loss: 0.1507 - val_accuracy: 0.9755 - val_loss: 0.0794\n",
            "Epoch 3/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9642 - loss: 0.1178 - val_accuracy: 0.9813 - val_loss: 0.0671\n",
            "Epoch 4/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9683 - loss: 0.1020 - val_accuracy: 0.9802 - val_loss: 0.0679\n",
            "Epoch 5/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9720 - loss: 0.0891 - val_accuracy: 0.9825 - val_loss: 0.0619\n",
            "Epoch 6/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9749 - loss: 0.0789 - val_accuracy: 0.9820 - val_loss: 0.0616\n",
            "Epoch 7/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9774 - loss: 0.0709 - val_accuracy: 0.9837 - val_loss: 0.0572\n",
            "Epoch 8/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9801 - loss: 0.0641 - val_accuracy: 0.9825 - val_loss: 0.0654\n",
            "Epoch 9/30\n",
            "844/844 - 28s - 33ms/step - accuracy: 0.9819 - loss: 0.0578 - val_accuracy: 0.9818 - val_loss: 0.0583\n",
            "Epoch 10/30\n",
            "844/844 - 28s - 33ms/step - accuracy: 0.9824 - loss: 0.0559 - val_accuracy: 0.9850 - val_loss: 0.0559\n",
            "Epoch 11/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9835 - loss: 0.0512 - val_accuracy: 0.9822 - val_loss: 0.0621\n",
            "Epoch 12/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9841 - loss: 0.0495 - val_accuracy: 0.9850 - val_loss: 0.0584\n",
            "Epoch 13/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9851 - loss: 0.0450 - val_accuracy: 0.9837 - val_loss: 0.0575\n",
            "Epoch 14/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9861 - loss: 0.0432 - val_accuracy: 0.9843 - val_loss: 0.0538\n",
            "Epoch 15/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9874 - loss: 0.0397 - val_accuracy: 0.9840 - val_loss: 0.0599\n",
            "Epoch 16/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9871 - loss: 0.0397 - val_accuracy: 0.9810 - val_loss: 0.0640\n",
            "Epoch 17/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9884 - loss: 0.0358 - val_accuracy: 0.9848 - val_loss: 0.0611\n",
            "Epoch 18/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9879 - loss: 0.0365 - val_accuracy: 0.9863 - val_loss: 0.0600\n",
            "Epoch 19/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9881 - loss: 0.0373 - val_accuracy: 0.9858 - val_loss: 0.0576\n",
            "Epoch 20/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9885 - loss: 0.0348 - val_accuracy: 0.9857 - val_loss: 0.0562\n",
            "Epoch 21/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9906 - loss: 0.0279 - val_accuracy: 0.9842 - val_loss: 0.0616\n",
            "Epoch 22/30\n",
            "844/844 - 26s - 31ms/step - accuracy: 0.9901 - loss: 0.0315 - val_accuracy: 0.9863 - val_loss: 0.0543\n",
            "Epoch 23/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9902 - loss: 0.0288 - val_accuracy: 0.9855 - val_loss: 0.0588\n",
            "Epoch 24/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9910 - loss: 0.0281 - val_accuracy: 0.9857 - val_loss: 0.0604\n",
            "Epoch 25/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9916 - loss: 0.0258 - val_accuracy: 0.9857 - val_loss: 0.0651\n",
            "Epoch 26/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9910 - loss: 0.0279 - val_accuracy: 0.9865 - val_loss: 0.0535\n",
            "Epoch 27/30\n",
            "844/844 - 27s - 32ms/step - accuracy: 0.9919 - loss: 0.0245 - val_accuracy: 0.9872 - val_loss: 0.0621\n",
            "Epoch 28/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9915 - loss: 0.0260 - val_accuracy: 0.9860 - val_loss: 0.0592\n",
            "Epoch 29/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9925 - loss: 0.0221 - val_accuracy: 0.9852 - val_loss: 0.0611\n",
            "Epoch 30/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9928 - loss: 0.0225 - val_accuracy: 0.9870 - val_loss: 0.0605\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9807 - loss: 0.0850\n",
            "\n",
            "Test Accuracy: 0.9847000241279602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Load and preprocess the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build a sequential model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Hidden layers with mix of activations and neurons\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer with Softmax\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with RMSprop (can also test with 'adam' or 'sgd')\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping for generalization\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"\\n Final Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct8dDKQ7ahJU",
        "outputId": "3c833389-e838-4621-d145-c9c84ef4ee76"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "844/844 - 29s - 34ms/step - accuracy: 0.9118 - loss: 0.2886 - val_accuracy: 0.9683 - val_loss: 0.1126\n",
            "Epoch 2/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9534 - loss: 0.1582 - val_accuracy: 0.9782 - val_loss: 0.0787\n",
            "Epoch 3/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9642 - loss: 0.1232 - val_accuracy: 0.9792 - val_loss: 0.0774\n",
            "Epoch 4/30\n",
            "844/844 - 26s - 31ms/step - accuracy: 0.9665 - loss: 0.1095 - val_accuracy: 0.9815 - val_loss: 0.0675\n",
            "Epoch 5/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9719 - loss: 0.0958 - val_accuracy: 0.9837 - val_loss: 0.0605\n",
            "Epoch 6/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9749 - loss: 0.0868 - val_accuracy: 0.9832 - val_loss: 0.0637\n",
            "Epoch 7/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9766 - loss: 0.0816 - val_accuracy: 0.9818 - val_loss: 0.0682\n",
            "Epoch 8/30\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9784 - loss: 0.0737 - val_accuracy: 0.9848 - val_loss: 0.0654\n",
            "Epoch 9/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9805 - loss: 0.0678 - val_accuracy: 0.9838 - val_loss: 0.0670\n",
            "Epoch 10/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9797 - loss: 0.0672 - val_accuracy: 0.9848 - val_loss: 0.0613\n",
            "Epoch 11/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9819 - loss: 0.0620 - val_accuracy: 0.9857 - val_loss: 0.0679\n",
            "Epoch 12/30\n",
            "844/844 - 42s - 49ms/step - accuracy: 0.9827 - loss: 0.0592 - val_accuracy: 0.9852 - val_loss: 0.0653\n",
            "Epoch 13/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9835 - loss: 0.0560 - val_accuracy: 0.9862 - val_loss: 0.0636\n",
            "Epoch 14/30\n",
            "844/844 - 26s - 31ms/step - accuracy: 0.9837 - loss: 0.0559 - val_accuracy: 0.9837 - val_loss: 0.0700\n",
            "Epoch 15/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9841 - loss: 0.0531 - val_accuracy: 0.9845 - val_loss: 0.0630\n",
            "Epoch 16/30\n",
            "844/844 - 26s - 30ms/step - accuracy: 0.9854 - loss: 0.0484 - val_accuracy: 0.9860 - val_loss: 0.0604\n",
            "Epoch 17/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9852 - loss: 0.0492 - val_accuracy: 0.9838 - val_loss: 0.0697\n",
            "Epoch 18/30\n",
            "844/844 - 26s - 31ms/step - accuracy: 0.9862 - loss: 0.0459 - val_accuracy: 0.9863 - val_loss: 0.0673\n",
            "Epoch 19/30\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9862 - loss: 0.0477 - val_accuracy: 0.9847 - val_loss: 0.0685\n",
            "Epoch 20/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9863 - loss: 0.0454 - val_accuracy: 0.9865 - val_loss: 0.0592\n",
            "Epoch 21/30\n",
            "844/844 - 42s - 50ms/step - accuracy: 0.9874 - loss: 0.0431 - val_accuracy: 0.9852 - val_loss: 0.0662\n",
            "Epoch 22/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9873 - loss: 0.0434 - val_accuracy: 0.9853 - val_loss: 0.0636\n",
            "Epoch 23/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9877 - loss: 0.0416 - val_accuracy: 0.9847 - val_loss: 0.0710\n",
            "Epoch 24/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9878 - loss: 0.0417 - val_accuracy: 0.9855 - val_loss: 0.0636\n",
            "Epoch 25/30\n",
            "844/844 - 40s - 47ms/step - accuracy: 0.9883 - loss: 0.0379 - val_accuracy: 0.9862 - val_loss: 0.0608\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9804 - loss: 0.0782\n",
            "\n",
            " Final Test Accuracy: 0.9844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Load and preprocess the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the model using Sequential API\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Hidden layers with mix of activations and neurons\n",
        "model.add(Dense(1024, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "\n",
        "# Output layer with Softmax\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with RMSprop (can also test with 'adam' or 'sgd')\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping for generalization\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"\\n Final Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXYKDhyZdsn9",
        "outputId": "40b396e9-ac04-430b-dca4-0ea497f414a8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "844/844 - 29s - 34ms/step - accuracy: 0.8893 - loss: 0.3706 - val_accuracy: 0.9613 - val_loss: 0.1406\n",
            "Epoch 2/30\n",
            "844/844 - 26s - 31ms/step - accuracy: 0.9384 - loss: 0.2116 - val_accuracy: 0.9685 - val_loss: 0.1095\n",
            "Epoch 3/30\n",
            "844/844 - 40s - 47ms/step - accuracy: 0.9514 - loss: 0.1660 - val_accuracy: 0.9748 - val_loss: 0.0881\n",
            "Epoch 4/30\n",
            "844/844 - 42s - 50ms/step - accuracy: 0.9597 - loss: 0.1386 - val_accuracy: 0.9762 - val_loss: 0.0870\n",
            "Epoch 5/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9674 - loss: 0.1132 - val_accuracy: 0.9803 - val_loss: 0.0705\n",
            "Epoch 6/30\n",
            "844/844 - 42s - 49ms/step - accuracy: 0.9695 - loss: 0.1056 - val_accuracy: 0.9792 - val_loss: 0.0723\n",
            "Epoch 7/30\n",
            "844/844 - 40s - 47ms/step - accuracy: 0.9720 - loss: 0.0952 - val_accuracy: 0.9808 - val_loss: 0.0782\n",
            "Epoch 8/30\n",
            "844/844 - 25s - 30ms/step - accuracy: 0.9746 - loss: 0.0856 - val_accuracy: 0.9817 - val_loss: 0.0696\n",
            "Epoch 9/30\n",
            "844/844 - 42s - 50ms/step - accuracy: 0.9762 - loss: 0.0821 - val_accuracy: 0.9837 - val_loss: 0.0654\n",
            "Epoch 10/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9781 - loss: 0.0751 - val_accuracy: 0.9820 - val_loss: 0.0757\n",
            "Epoch 11/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9793 - loss: 0.0705 - val_accuracy: 0.9845 - val_loss: 0.0627\n",
            "Epoch 12/30\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9808 - loss: 0.0654 - val_accuracy: 0.9833 - val_loss: 0.0667\n",
            "Epoch 13/30\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9817 - loss: 0.0625 - val_accuracy: 0.9842 - val_loss: 0.0686\n",
            "Epoch 14/30\n",
            "844/844 - 42s - 49ms/step - accuracy: 0.9826 - loss: 0.0589 - val_accuracy: 0.9845 - val_loss: 0.0611\n",
            "Epoch 15/30\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9829 - loss: 0.0595 - val_accuracy: 0.9822 - val_loss: 0.0732\n",
            "Epoch 16/30\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9844 - loss: 0.0557 - val_accuracy: 0.9838 - val_loss: 0.0681\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9735 - loss: 0.0979\n",
            "\n",
            " Final Test Accuracy: 0.9786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Load and preprocess the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the sequential model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(28, 28)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Hidden layers\n",
        "model.add(Dense(1024, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compiler\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=50,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"\\nFinal Test Accuracy (SGD): {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QnIY1xEnZWt",
        "outputId": "bbae8e84-1c7f-4af7-ec2b-2cca78a29144"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "844/844 - 28s - 33ms/step - accuracy: 0.8739 - loss: 0.4190 - val_accuracy: 0.9577 - val_loss: 0.1485\n",
            "Epoch 2/50\n",
            "844/844 - 39s - 47ms/step - accuracy: 0.9230 - loss: 0.2567 - val_accuracy: 0.9660 - val_loss: 0.1204\n",
            "Epoch 3/50\n",
            "844/844 - 42s - 49ms/step - accuracy: 0.9381 - loss: 0.2074 - val_accuracy: 0.9733 - val_loss: 0.1040\n",
            "Epoch 4/50\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9458 - loss: 0.1789 - val_accuracy: 0.9722 - val_loss: 0.0961\n",
            "Epoch 5/50\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9502 - loss: 0.1647 - val_accuracy: 0.9738 - val_loss: 0.0932\n",
            "Epoch 6/50\n",
            "844/844 - 42s - 49ms/step - accuracy: 0.9552 - loss: 0.1508 - val_accuracy: 0.9750 - val_loss: 0.0887\n",
            "Epoch 7/50\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9581 - loss: 0.1396 - val_accuracy: 0.9757 - val_loss: 0.0786\n",
            "Epoch 8/50\n",
            "844/844 - 25s - 29ms/step - accuracy: 0.9603 - loss: 0.1289 - val_accuracy: 0.9780 - val_loss: 0.0786\n",
            "Epoch 9/50\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9637 - loss: 0.1185 - val_accuracy: 0.9780 - val_loss: 0.0782\n",
            "Epoch 10/50\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9655 - loss: 0.1123 - val_accuracy: 0.9785 - val_loss: 0.0712\n",
            "Epoch 11/50\n",
            "844/844 - 25s - 29ms/step - accuracy: 0.9669 - loss: 0.1077 - val_accuracy: 0.9812 - val_loss: 0.0703\n",
            "Epoch 12/50\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9692 - loss: 0.1012 - val_accuracy: 0.9802 - val_loss: 0.0691\n",
            "Epoch 13/50\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9704 - loss: 0.0957 - val_accuracy: 0.9832 - val_loss: 0.0611\n",
            "Epoch 14/50\n",
            "844/844 - 24s - 29ms/step - accuracy: 0.9724 - loss: 0.0902 - val_accuracy: 0.9825 - val_loss: 0.0636\n",
            "Epoch 15/50\n",
            "844/844 - 42s - 49ms/step - accuracy: 0.9719 - loss: 0.0895 - val_accuracy: 0.9813 - val_loss: 0.0652\n",
            "Epoch 16/50\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9745 - loss: 0.0832 - val_accuracy: 0.9830 - val_loss: 0.0605\n",
            "Epoch 17/50\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9735 - loss: 0.0846 - val_accuracy: 0.9832 - val_loss: 0.0625\n",
            "Epoch 18/50\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9752 - loss: 0.0794 - val_accuracy: 0.9825 - val_loss: 0.0617\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9732 - loss: 0.0845\n",
            "\n",
            "Final Test Accuracy (SGD): 0.9780\n"
          ]
        }
      ]
    }
  ]
}